# Example ai-agent.yaml (General)
# Copy this to config/ai-agent.yaml and customize for your environment.
#
# Choose one of the example variants as a starting point:
#  - config/ai-agent.local.yaml            (Local-only pipeline)
#  - config/ai-agent.cloud-openai.yaml     (Cloud-only OpenAI pipeline)
#  - config/ai-agent.hybrid.yaml           (Local STT + OpenAI LLM + Deepgram TTS)
#  - config/ai-agent.openai-agent.yaml     (Monolithic OpenAI Realtime agent)
#  - config/ai-agent.deepgram-agent.yaml   (Monolithic Deepgram Voice Agent)
#
# Secrets come from .env (see .env.example). YAML values may reference env using ${NAME}.

# Use OpenAI Realtime as the safe monolithic fallback; pipelines remain the primary path
default_provider: "openai_realtime"

# Example pipelines. Pick one and set active_pipeline accordingly.
pipelines:
  # Cloud OpenAI across STT/LLM/TTS
  cloud_openai:
    stt: openai_stt
    llm: openai_llm
    tts: openai_tts
    options:
      stt:
        base_url: "wss://api.openai.com/v1/realtime"
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o-realtime-preview-2024-12-17"
      tts:
        base_url: "https://api.openai.com/v1/audio/speech"
        format:
          encoding: linear16
          sample_rate: 24000
  # Local provider for all components
  local_only:
    stt: local_stt
    llm: local_llm
    tts: local_tts
    options:
      stt:
        chunk_ms: 320
        streaming: true
        stream_format: "pcm16_16k"
      llm:
        temperature: 0.4
        max_tokens: 64
      tts:
        format:
          encoding: ulaw
          sample_rate: 8000
  # Local STT + OpenAI LLM + Deepgram TTS
  hybrid_deepgram_openai:
    stt: local_stt
    llm: openai_llm
    tts: deepgram_tts
    options:
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o-mini"
      tts:
        base_url: "https://api.deepgram.com"
        voice: "aura-asteria-en"
        format:
          encoding: mulaw
          sample_rate: 8000

# Select an active pipeline when using the pipeline path
active_pipeline: cloud_openai

# Transport modes
audio_transport: "audiosocket"   # audiosocket | externalmedia
downstream_mode: "stream"        # stream | file

# AudioSocket listener (when audio_transport=audiosocket)
audiosocket:
  host: "0.0.0.0"
  port: 8090
  format: "ulaw"            # ulaw | slin16 (8 kHz)

# ExternalMedia RTP (when audio_transport=externalmedia)
external_media:
  rtp_host: "0.0.0.0"
  rtp_port: 18080
  codec: "ulaw"
  direction: "both"

# Optional VAD/barge-in and streaming tuning
barge_in:
  enabled: false
  initial_protection_ms: 400    # Drop inbound during agent intro; 200–600 ms. Higher = less echo, more delay.
  min_ms: 400                   # Sustained speech required to trigger; 250–600 ms. Lower = more sensitive.
  energy_threshold: 1800        # RMS threshold for speech detection; 1000–3000. Raise on noisy lines.
  cooldown_ms: 1000             # Ignore retriggers for this period after one fires; 500–1500 ms.
  post_tts_end_protection_ms: 250  # Guard after TTS ends to avoid clipping callers; 250–500 ms.

streaming:
  sample_rate: 8000
  jitter_buffer_ms: 100         # 80–150 ms. Higher = more robust to jitter, slightly more latency.
  keepalive_interval_ms: 5000
  connection_timeout_ms: 10000
  fallback_timeout_ms: 8000
  chunk_size_ms: 20
  min_start_ms: 300             # 250–400 ms. Warm-up buffer; too low risks underruns.
  low_watermark_ms: 200         # Pause when buffer dips below this; raise if underruns occur.
  provider_grace_ms: 500        # Absorb late chunks after cleanup; avoids tail-chop.
  logging_level: "info"

# VAD: add a `vad:` block if you need utterance segmentation control; see docs/Configuration-Reference.md

# Providers (secrets from .env)
providers:
  local:
    enabled: true
    ws_url: "${LOCAL_WS_URL:-ws://127.0.0.1:8765}"
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=5.0}
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
  openai_realtime:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4o-realtime-preview-2024-12-17"
    voice: "alloy"
    base_url: "wss://api.openai.com/v1/realtime"
  deepgram:
    enabled: true
    api_key: "${DEEPGRAM_API_KEY}"
    model: "nova-2-general"
    tts_model: "aura-asteria-en"

# Canonical LLM defaults (used by providers/pipelines unless overridden)
llm:
  initial_greeting: "Hello, how can I help you today?"
  prompt: "You are a concise and helpful voice assistant. Keep replies under 20 words unless asked for detail."
  model: "gpt-4o"
