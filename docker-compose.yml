services:
  ai-engine:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai_engine
    user: "appuser"
    volumes:
      - ./src:/app/src
      - ./main.py:/app/main.py
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ./models:/app/models
      - ./audio/greeting.ulaw:/audio/greeting.ulaw
      - /mnt/asterisk_media:/mnt/asterisk_media
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
    tty: true
    stdin_open: true
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      local-ai-server:
        condition: service_started

  local-ai-server:
    build:
      context: ./local_ai_server
      dockerfile: Dockerfile
    container_name: local_ai_server
    env_file:
      - .env
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - LOCAL_STT_IDLE_MS=3000
      - LOCAL_LLM_INFER_TIMEOUT_SEC=${LOCAL_LLM_INFER_TIMEOUT_SEC:-12}
      - LOCAL_LLM_MODEL_PATH=${LOCAL_LLM_MODEL_PATH:-/app/models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf}
      - LOCAL_LLM_THREADS=${LOCAL_LLM_THREADS:-16}
      - LOCAL_LLM_CONTEXT=${LOCAL_LLM_CONTEXT:-4096}
      - LOCAL_LLM_BATCH=${LOCAL_LLM_BATCH:-256}
      - LOCAL_LLM_MAX_TOKENS=${LOCAL_LLM_MAX_TOKENS:-32}
      - LOCAL_LLM_TEMPERATURE=${LOCAL_LLM_TEMPERATURE:-0.2}
      - LOCAL_LLM_TOP_P=${LOCAL_LLM_TOP_P:-0.85}
      - LOCAL_LLM_REPEAT_PENALTY=${LOCAL_LLM_REPEAT_PENALTY:-1.05}
      - LOCAL_STT_MODEL_PATH=${LOCAL_STT_MODEL_PATH:-/app/models/stt/vosk-model-en-us-0.22}
      - LOCAL_TTS_MODEL_PATH=${LOCAL_TTS_MODEL_PATH:-/app/models/tts/en_US-lessac-medium.onnx}
      - LOCAL_LLM_USE_MLOCK=${LOCAL_LLM_USE_MLOCK:-0}
    tty: true
    stdin_open: true
    restart: unless-stopped
    network_mode: "host"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import socket; s=socket.socket(); s.settimeout(1); s.connect(('127.0.0.1',8765)); s.close()\""]
      interval: 10s
      timeout: 5s
      retries: 180
      start_period: 60s
