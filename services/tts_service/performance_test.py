#!/usr/bin/env python3
"""
TTS Service Performance Tests

Tests performance metrics for the TTS service including:
- Audio synthesis speed and throughput
- Memory usage during processing
- File I/O performance
- Redis message publishing performance
- Concurrent request handling
"""

import asyncio
import json
import os
import time
import uuid
import psutil
import tracemalloc
from pathlib import Path
from typing import Dict, Any, List
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Performance test configuration
PERF_CONFIG = {
    'redis_url': 'redis://localhost:6379',
    'openai_api_key': os.getenv('OPENAI_API_KEY'),
    'tts_voice': 'alloy',
    'tts_base_directory': '/shared/audio',
    'concurrent_requests': 10,
    'test_duration_seconds': 60,
    'text_samples': [
        "Hello, this is a short test message.",
        "This is a medium length test message that contains more words and should take longer to process.",
        "This is a very long test message that contains many words and should provide a good test of the TTS service performance under load with realistic text content that might be generated by an AI language model in a real conversation scenario."
    ]
}

class TTSPerformanceTester:
    """TTS service performance tester."""
    
    def __init__(self):
        self.metrics = {
            'synthesis_times': [],
            'file_creation_times': [],
            'redis_publish_times': [],
            'memory_usage': [],
            'concurrent_throughput': 0,
            'error_count': 0,
            'total_requests': 0
        }
        self.test_audio_files = []
        self.redis_client = None
        
    async def setup(self):
        """Set up performance test environment."""
        logger.info("Setting up TTS performance test environment...")
        
        # Create test directory
        self.test_dir = Path(PERF_CONFIG['tts_base_directory'])
        self.test_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Redis client
        try:
            import redis.asyncio as redis
            self.redis_client = redis.from_url(PERF_CONFIG['redis_url'])
            await self.redis_client.ping()
            logger.info("✅ Redis connection established")
        except Exception as e:
            logger.error(f"❌ Redis connection failed: {e}")
            return False
        
        # Start memory tracking
        tracemalloc.start()
        
        return True
    
    async def cleanup(self):
        """Clean up performance test environment."""
        logger.info("Cleaning up performance test environment...")
        
        # Remove test audio files
        for file_path in self.test_audio_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
            except Exception as e:
                logger.warning(f"Failed to remove {file_path}: {e}")
        
        # Close Redis connection
        if self.redis_client:
            await self.redis_client.close()
        
        # Stop memory tracking
        tracemalloc.stop()
    
    async def test_single_synthesis_performance(self):
        """Test single audio synthesis performance."""
        logger.info("Testing single synthesis performance...")
        
        try:
            from openai_tts_client import OpenAITTSClient
            
            client = OpenAITTSClient(
                api_key=PERF_CONFIG['openai_api_key'],
                voice=PERF_CONFIG['tts_voice']
            )
            
            # Test with different text lengths
            for i, text in enumerate(PERF_CONFIG['text_samples']):
                logger.info(f"Testing synthesis {i+1}/{len(PERF_CONFIG['text_samples'])}")
                
                start_time = time.time()
                result = await client.synthesize_text(text)
                end_time = time.time()
                
                synthesis_time = end_time - start_time
                self.metrics['synthesis_times'].append({
                    'text_length': len(text),
                    'synthesis_time': synthesis_time,
                    'audio_size': len(result.audio_data),
                    'duration_ms': result.duration_ms
                })
                
                logger.info(f"  Synthesis time: {synthesis_time:.3f}s")
                logger.info(f"  Audio size: {len(result.audio_data)} bytes")
                logger.info(f"  Duration: {result.duration_ms}ms")
                
                # Track memory usage
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                self.metrics['memory_usage'].append(memory_mb)
                
        except Exception as e:
            logger.error(f"❌ Single synthesis performance test failed: {e}")
            self.metrics['error_count'] += 1
    
    async def test_file_management_performance(self):
        """Test file management performance."""
        logger.info("Testing file management performance...")
        
        try:
            from audio_file_manager import AudioFileManager
            
            manager = AudioFileManager(
                base_directory=PERF_CONFIG['tts_base_directory'],
                ttl_seconds=3600
            )
            
            # Test file creation performance
            test_audio_data = b"fake_audio_data_for_performance_testing" * 1000  # 1KB of test data
            
            for i in range(20):  # Create 20 test files
                start_time = time.time()
                
                file_info = await manager.save_audio_file(
                    audio_data=test_audio_data,
                    text=f"Performance test file {i}",
                    original_format='wav',
                    metadata={'test_id': str(uuid.uuid4())}
                )
                
                end_time = time.time()
                
                file_creation_time = end_time - start_time
                self.metrics['file_creation_times'].append({
                    'file_size': len(test_audio_data),
                    'creation_time': file_creation_time,
                    'file_id': file_info.file_id
                })
                
                # Track for cleanup
                self.test_audio_files.append(str(file_info.file_path))
            
            # Test file cleanup performance
            start_time = time.time()
            await manager.cleanup_expired_files()
            end_time = time.time()
            
            cleanup_time = end_time - start_time
            logger.info(f"File cleanup time: {cleanup_time:.3f}s")
            
        except Exception as e:
            logger.error(f"❌ File management performance test failed: {e}")
            self.metrics['error_count'] += 1
    
    async def test_redis_publishing_performance(self):
        """Test Redis message publishing performance."""
        logger.info("Testing Redis publishing performance...")
        
        try:
            # Test message publishing speed
            test_message = {
                'channel_id': 'perf_test_channel',
                'file_id': str(uuid.uuid4()),
                'file_path': '/shared/audio/test.wav',
                'text': 'Performance test message',
                'timestamp': time.time()
            }
            
            # Publish 100 messages and measure time
            start_time = time.time()
            
            for i in range(100):
                await self.redis_client.publish(
                    "tts:audio:ready",
                    json.dumps(test_message)
                )
            
            end_time = time.time()
            
            total_time = end_time - start_time
            avg_time_per_message = total_time / 100
            
            self.metrics['redis_publish_times'].append({
                'total_messages': 100,
                'total_time': total_time,
                'avg_time_per_message': avg_time_per_message,
                'messages_per_second': 100 / total_time
            })
            
            logger.info(f"Redis publishing: {100/total_time:.1f} messages/second")
            
        except Exception as e:
            logger.error(f"❌ Redis publishing performance test failed: {e}")
            self.metrics['error_count'] += 1
    
    async def test_concurrent_throughput(self):
        """Test concurrent request throughput."""
        logger.info("Testing concurrent throughput...")
        
        try:
            from openai_tts_client import OpenAITTSClient
            
            client = OpenAITTSClient(
                api_key=PERF_CONFIG['openai_api_key'],
                voice=PERF_CONFIG['tts_voice']
            )
            
            # Create concurrent requests
            concurrent_requests = PERF_CONFIG['concurrent_requests']
            test_text = "Concurrent performance test message"
            
            start_time = time.time()
            
            # Run concurrent synthesis
            tasks = [
                client.synthesize_text(f"{test_text} {i}")
                for i in range(concurrent_requests)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            
            # Calculate metrics
            total_time = end_time - start_time
            successful_results = [r for r in results if not isinstance(r, Exception)]
            success_rate = len(successful_results) / concurrent_requests
            
            self.metrics['concurrent_throughput'] = {
                'concurrent_requests': concurrent_requests,
                'successful_requests': len(successful_results),
                'success_rate': success_rate,
                'total_time': total_time,
                'requests_per_second': concurrent_requests / total_time
            }
            
            logger.info(f"Concurrent throughput: {concurrent_requests/total_time:.1f} requests/second")
            logger.info(f"Success rate: {success_rate:.1%}")
            
        except Exception as e:
            logger.error(f"❌ Concurrent throughput test failed: {e}")
            self.metrics['error_count'] += 1
    
    async def test_memory_usage_under_load(self):
        """Test memory usage under sustained load."""
        logger.info("Testing memory usage under load...")
        
        try:
            from tts_service import TTSService, TTSServiceConfig
            
            config = TTSServiceConfig(
                redis_url=PERF_CONFIG['redis_url'],
                openai_api_key=PERF_CONFIG['openai_api_key'],
                tts_voice=PERF_CONFIG['tts_voice'],
                tts_base_directory=PERF_CONFIG['tts_base_directory'],
                tts_file_ttl=3600,
                tts_enable_fallback=False  # Disable fallback for performance test
            )
            
            service = TTSService(config)
            await service.start()
            
            # Monitor memory usage during sustained load
            test_duration = PERF_CONFIG['test_duration_seconds']
            start_time = time.time()
            request_count = 0
            
            while time.time() - start_time < test_duration:
                # Send test request
                test_llm_response = {
                    'channel_id': f'perf_channel_{request_count}',
                    'text': f"Memory test message {request_count}",
                    'model_used': 'gpt-4o',
                    'tokens_used': 10
                }
                
                await service._handle_llm_response(json.dumps(test_llm_response))
                
                # Record memory usage
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                self.metrics['memory_usage'].append({
                    'timestamp': time.time() - start_time,
                    'memory_mb': memory_mb,
                    'request_count': request_count
                })
                
                request_count += 1
                self.metrics['total_requests'] = request_count
                
                # Small delay to prevent overwhelming
                await asyncio.sleep(0.1)
            
            await service.stop()
            
            logger.info(f"Processed {request_count} requests in {test_duration}s")
            logger.info(f"Average memory usage: {sum(m['memory_mb'] for m in self.metrics['memory_usage']) / len(self.metrics['memory_usage']):.1f} MB")
            
        except Exception as e:
            logger.error(f"❌ Memory usage under load test failed: {e}")
            self.metrics['error_count'] += 1
    
    def analyze_performance_metrics(self):
        """Analyze and report performance metrics."""
        logger.info("\n" + "="*60)
        logger.info("TTS SERVICE PERFORMANCE ANALYSIS")
        logger.info("="*60)
        
        # Synthesis performance
        if self.metrics['synthesis_times']:
            avg_synthesis_time = sum(t['synthesis_time'] for t in self.metrics['synthesis_times']) / len(self.metrics['synthesis_times'])
            logger.info(f"Average synthesis time: {avg_synthesis_time:.3f}s")
            
            # Group by text length
            short_texts = [t for t in self.metrics['synthesis_times'] if t['text_length'] < 50]
            medium_texts = [t for t in self.metrics['synthesis_times'] if 50 <= t['text_length'] < 100]
            long_texts = [t for t in self.metrics['synthesis_times'] if t['text_length'] >= 100]
            
            if short_texts:
                avg_short = sum(t['synthesis_time'] for t in short_texts) / len(short_texts)
                logger.info(f"Short texts (<50 chars): {avg_short:.3f}s average")
            
            if medium_texts:
                avg_medium = sum(t['synthesis_time'] for t in medium_texts) / len(medium_texts)
                logger.info(f"Medium texts (50-100 chars): {avg_medium:.3f}s average")
            
            if long_texts:
                avg_long = sum(t['synthesis_time'] for t in long_texts) / len(long_texts)
                logger.info(f"Long texts (100+ chars): {avg_long:.3f}s average")
        
        # File creation performance
        if self.metrics['file_creation_times']:
            avg_file_creation = sum(t['creation_time'] for t in self.metrics['file_creation_times']) / len(self.metrics['file_creation_times'])
            logger.info(f"Average file creation time: {avg_file_creation:.3f}s")
        
        # Redis publishing performance
        if self.metrics['redis_publish_times']:
            for perf in self.metrics['redis_publish_times']:
                logger.info(f"Redis publishing: {perf['messages_per_second']:.1f} msg/s")
        
        # Concurrent throughput
        if self.metrics['concurrent_throughput']:
            throughput = self.metrics['concurrent_throughput']
            logger.info(f"Concurrent throughput: {throughput['requests_per_second']:.1f} req/s")
            logger.info(f"Success rate: {throughput['success_rate']:.1%}")
        
        # Memory usage
        if self.metrics['memory_usage']:
            if isinstance(self.metrics['memory_usage'][0], dict):
                # Memory usage under load
                memory_values = [m['memory_mb'] for m in self.metrics['memory_usage']]
            else:
                # Simple memory usage
                memory_values = self.metrics['memory_usage']
            
            avg_memory = sum(memory_values) / len(memory_values)
            max_memory = max(memory_values)
            min_memory = min(memory_values)
            
            logger.info(f"Memory usage - Avg: {avg_memory:.1f}MB, Min: {min_memory:.1f}MB, Max: {max_memory:.1f}MB")
        
        # Error rate
        total_operations = self.metrics['total_requests'] + len(self.metrics['synthesis_times']) + len(self.metrics['file_creation_times'])
        error_rate = self.metrics['error_count'] / total_operations if total_operations > 0 else 0
        logger.info(f"Error rate: {error_rate:.1%}")
        
        logger.info("="*60)
    
    async def run_performance_tests(self):
        """Run all performance tests."""
        logger.info("Starting TTS service performance tests...")
        
        # Setup
        if not await self.setup():
            logger.error("Failed to setup performance test environment")
            return False
        
        try:
            # Run performance tests
            await self.test_single_synthesis_performance()
            await self.test_file_management_performance()
            await self.test_redis_publishing_performance()
            await self.test_concurrent_throughput()
            await self.test_memory_usage_under_load()
            
            # Analyze results
            self.analyze_performance_metrics()
            
            return True
            
        finally:
            await self.cleanup()

async def main():
    """Main performance test runner."""
    tester = TTSPerformanceTester()
    success = await tester.run_performance_tests()
    return 0 if success else 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
