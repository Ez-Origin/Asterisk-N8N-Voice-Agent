# Hybrid configuration: Local STT + OpenAI LLM + Deepgram TTS
# Copy to config/ai-agent.yaml for a hybrid setup mixing local and cloud.

default_provider: "openai_realtime"  # Monolithic fallback; pipeline is primary

pipelines:
  hybrid:
    stt: local_stt
    llm: openai_llm
    tts: deepgram_tts
    options:
      stt:
        chunk_ms: 320
        streaming: true
        stream_format: "pcm16_16k"
      llm:
        base_url: "https://api.openai.com/v1"
        chat_model: "gpt-4o-mini"
        temperature: 0.6
        max_tokens: 120
        timeout_sec: 15.0
      tts:
        base_url: "https://api.deepgram.com"
        voice: "aura-asteria-en"
        format:
          encoding: mulaw
          sample_rate: 8000

active_pipeline: hybrid

audio_transport: "audiosocket"
downstream_mode: "stream"

audiosocket:
  host: "0.0.0.0"
  port: 8090
  format: "ulaw"

# Telephony-tuned barge-in and VAD to avoid TTS → STT echo and speed up finalization
barge_in:
  enabled: true
  initial_protection_ms: 350
  min_ms: 300
  energy_threshold: 1800
  cooldown_ms: 800
  post_tts_end_protection_ms: 250

vad:
  # WebRTC VAD settings tuned for telephony
  webrtc_aggressiveness: 1
  webrtc_start_frames: 2
  webrtc_end_silence_frames: 25   # ~500 ms end-silence
  # Utterance settings — allow short 1–2 s replies
  min_utterance_duration_ms: 2000
  max_utterance_duration_ms: 10000
  utterance_padding_ms: 200
  # Fallback settings — faster fallback to ensure progress
  fallback_enabled: true
  fallback_interval_ms: 2000
  fallback_buffer_size: 64000

providers:
  local:
    enabled: true
    ws_url: "${LOCAL_WS_URL:-ws://127.0.0.1:8765}"
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=5.0}
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
  openai_realtime:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
  deepgram:
    enabled: true
    api_key: "${DEEPGRAM_API_KEY}"
