version: "3.8"
services:
  local-ai-server:
    environment:
      LOCAL_LLM_INFER_TIMEOUT_SEC: "35"
      LOCAL_LLM_MODEL_PATH: "/app/models/llm/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf"
      LOCAL_LLM_CONTEXT: "256"
      LOCAL_LLM_THREADS: "4"
      LOCAL_LLM_BATCH: "8"
      LOCAL_LLM_MAX_TOKENS: "8"
      LOCAL_LLM_TEMPERATURE: "0.2"
      LOCAL_LLM_TOP_P: "0.8"
      LOCAL_LLM_REPEAT_PENALTY: "1.0"
      OMP_NUM_THREADS: "1"
