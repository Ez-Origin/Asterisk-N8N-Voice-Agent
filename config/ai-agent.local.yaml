# Local-only pipeline configuration
# Copy to config/ai-agent.yaml for a fully local setup (no cloud keys needed).

default_provider: "openai_realtime"  # Monolithic fallback (not used when pipeline forced)

pipelines:
  local_only:
    stt: local_stt
    llm: local_llm
    tts: local_tts
    options:
      stt:
        chunk_ms: 320
        streaming: true
        stream_format: "pcm16_16k"
      llm:
        temperature: 0.4
        max_tokens: 64
        llm_response_timeout_sec: 60.0
      tts:
        format:
          encoding: ulaw
          sample_rate: 8000

active_pipeline: local_only

audio_transport: "audiosocket"
downstream_mode: "stream"

barge_in:
  enabled: true
  initial_protection_ms: 350
  min_ms: 300
  energy_threshold: 1500
  cooldown_ms: 800
  post_tts_end_protection_ms: 250

vad:
  # WebRTC VAD settings tuned for telephony
  webrtc_aggressiveness: 1
  webrtc_start_frames: 2
  webrtc_end_silence_frames: 25   # ~500 ms end-silence
  
  # Utterance settings — allow short 1–2 s replies
  min_utterance_duration_ms: 2000
  max_utterance_duration_ms: 10000
  utterance_padding_ms: 200
  
  # Fallback settings — faster fallback to ensure progress
  fallback_enabled: true
  fallback_interval_ms: 2000
  fallback_buffer_size: 64000

audiosocket:
  host: "0.0.0.0"
  port: 8090
  format: "ulaw"

providers:
  local:
    enabled: true
    ws_url: "${LOCAL_WS_URL:-ws://127.0.0.1:8765}"
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=5.0}
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
