# Optimized Local LLM Configuration for Llama-2-13B on 39GB RAM / 16 cores
# Goal: <30 second response times for pipeline verification
#
# Usage: Append these to server .env, then restart local-ai-server

# === CRITICAL PERFORMANCE SETTINGS ===

# Timeout - allow 30s for inference (was 12s)
LOCAL_LLM_INFER_TIMEOUT_SEC=30

# Max tokens - DRASTICALLY reduce for speed (was 32, now 16)
# Each token takes ~1-2 seconds with 13B model
LOCAL_LLM_MAX_TOKENS=16

# Context window - reduce to speed up prompt processing (was 4096)
# Smaller context = faster inference
LOCAL_LLM_CONTEXT=512

# Batch size - increase for better throughput (was 256)
LOCAL_LLM_BATCH=512

# Threads - match CPU cores
LOCAL_LLM_THREADS=16

# Temperature - lower = more deterministic = faster (was 0.2)
LOCAL_LLM_TEMPERATURE=0.1

# Top-p sampling - more restrictive = faster
LOCAL_LLM_TOP_P=0.75

# Repeat penalty - keep low to avoid extra processing
LOCAL_LLM_REPEAT_PENALTY=1.02

# Memory locking - pin model to RAM (prevents swapping)
# WARNING: Requires container privileges
LOCAL_LLM_USE_MLOCK=1

# === ALTERNATIVE: SWITCH TO FASTER MODEL ===
# If 30s is still too slow, uncomment this to use smaller model:
# LOCAL_LLM_MODEL_PATH=/app/models/llm/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
# With TinyLlama expect: 5-10 second responses

# === EXTREME SPEED MODE (for testing only) ===
# Uncomment these for absolute minimum latency (quality will suffer):
# LOCAL_LLM_MAX_TOKENS=8
# LOCAL_LLM_CONTEXT=256
# LOCAL_LLM_BATCH=1024
