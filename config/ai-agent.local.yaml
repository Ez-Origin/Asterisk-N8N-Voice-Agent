# Local-only pipeline configuration
# Copy to config/ai-agent.yaml for a fully local setup (no cloud keys needed).

default_provider: "openai_realtime"  # Monolithic fallback (not used when pipeline forced)

pipelines:
  local_only:
    stt: local_stt
    llm: local_llm
    tts: local_tts
    options:
      stt:
        chunk_ms: 320
        streaming: true
        stream_format: "pcm16_16k"
      llm:
        temperature: 0.4
        max_tokens: 64
        llm_response_timeout_sec: 60.0
      tts:
        format:
          encoding: ulaw
          sample_rate: 8000

active_pipeline: local_only

audio_transport: "audiosocket"
downstream_mode: "stream"

barge_in:
  enabled: true
  initial_protection_ms: 350    # Drop inbound during agent intro; 200–600 ms. Higher = less echo, more delay.
  min_ms: 300                   # Sustained speech required to trigger; 250–600 ms. Lower = more sensitive.
  energy_threshold: 1500        # RMS threshold for speech detection; 1000–3000. Raise on noisy lines.
  cooldown_ms: 800              # Ignore retriggers for this period after one fires; 500–1500 ms.
  post_tts_end_protection_ms: 250  # Guard after TTS ends to avoid clipping callers; 250–500 ms.

vad:
  # WebRTC VAD settings tuned for telephony
  webrtc_aggressiveness: 1
  webrtc_start_frames: 2
  webrtc_end_silence_frames: 25   # ~500 ms end-silence
  # See docs/Configuration-Reference.md for full VAD option impacts and presets
  
  # Utterance settings — allow short 1–2 s replies
  min_utterance_duration_ms: 2000
  max_utterance_duration_ms: 10000
  utterance_padding_ms: 200
  
  # Fallback settings — faster fallback to ensure progress
  fallback_enabled: true
  fallback_interval_ms: 2000
  fallback_buffer_size: 64000

audiosocket:
  host: "0.0.0.0"
  port: 8090
  format: "ulaw"

providers:
  local:
    enabled: true
    ws_url: "${LOCAL_WS_URL:-ws://127.0.0.1:8765}"
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=5.0}
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}

# Canonical LLM defaults (used by providers/pipelines unless overridden)
llm:
  initial_greeting: "Hello, how can I help you today?"
  prompt: "You are a concise and helpful voice assistant. Keep replies under 20 words unless asked for detail."
  model: "gpt-4o"
